{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/papailth/fashion_product_recommendation_engine/blob/main/Shop_The_Look_Despotis_Papailiou_NO_OUTPUT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnA2PhNJpO5t"
      },
      "source": [
        "> <a class=\"anchor\" id=\"00\"></a> \n",
        "\n",
        "\n",
        "\n",
        "> <img src=\"https://i.ibb.co/mCs7nGx/BA.jpg\" alt=\"Drawing\"  style=\"width: 1040px;\"/>  <br />\n",
        ">\n",
        ">\n",
        "> <img src=\"https://i.ibb.co/8jhs2qL/Shop-The-Look-8-24-2022.png\" alt=\"Drawing\"  style=\"width: 950px;\"/> \n",
        "<br />\n",
        "> <img src=\"https://i.ibb.co/dgR09sy/shop-the-look-n-final.gif\" alt=\"Drawing\" style=\"width: 862px;\"/> \n",
        "> <br />\n",
        "><br />\n",
        "><b> \n",
        "    <font size=\"+1.3\">\n",
        "> <span style='font-family:Verdana '>üöÄ Project: Shop The Look 2022 by Whiskey Team <br />\n",
        "> <span style='font-family:Verdana '>üöÄ Msc in Business Analytics @ AEUB  <br />\n",
        ">  <span style='font-family:Verdana '>üöÄCourse: Machine Learning & Content Analytics - Part Time  <br />\n",
        ">  <span style='font-family:Verdana '>üöÄ Professors: H.Papageorgiou | G. Perakis <br />\n",
        ">  <span style='font-family:Verdana '>üöÄ Students: S.Despotis| T.Papailiou <br />\n",
        "        <font/>\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKu1RDEeLpMH"
      },
      "source": [
        " > <img src=\"https://i.ibb.co/thkLqxq/contents.png\" alt=\"Drawing\" style=\"width: 380px;\"/> <br />\n",
        "><br />\n",
        "> <img src=\"https://i.ibb.co/5ryvTFy/7878788.png\" alt=\"Drawing\" style=\"width: 862px;\"/>\n",
        "> <br />\n",
        "> <br />\n",
        "> <br />\n",
        "<font size=\"+2\">\n",
        ">‚úÖ 00 - [Introduction](#0) <br />\n",
        ">‚úÖ 01 - [Explanatory Data Analysis](#1) <br />\n",
        ">‚úÖ 02 - [Gender Classification](#2) <br />\n",
        ">‚úÖ 03 - [Object Detection](#3) <br />\n",
        ">‚úÖ 04 - [Extracting Items](#4) <br />\n",
        ">‚úÖ 05 - [Embeddings Generation](#5) <br />\n",
        ">‚úÖ 06 - [Final Pipleline](#6) <br />\n",
        "    </font>\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE0jG0tYLpMI"
      },
      "source": [
        "<a class=\"anchor\" id=\"0\"></a> \n",
        "\n",
        "> <img src=\"https://i.ibb.co/PDbYHfh/introduction.png\" alt=\"Drawing\" style=\"width: 480px;\"/> <br />\n",
        "><br />\n",
        "> <img src=\"https://institute.careerguide.com/wp-content/uploads/2020/10/b1203194834f68bd24fc0aaaf6a74103.gif\" alt=\"Drawing\" style=\"width: 662px;\"/>\n",
        "> <br />\n",
        "> <br />\n",
        "> <br />\n",
        "<font size=\"+1\">\n",
        "Every day, consumers waste a lot of time on searching for clothes, especially when they know exactly what they want. Since fashion is about the look ‚Äì combining multiple items together ‚Äì we thought it would be a great idea to give to users a faster tool to search outfits online, rather than typing multiple textual keywords in search engines. In our case, the adage ‚ÄúA picture is worth a thousand words‚Äù was valid, since the images were the ‚Äúkey‚Äù solution to our problem. Due to the fact that people are familiar with snaping pictures (e.g. for their social media) and also pictures contain a variety of information, we focused on providing an easy going solution to the users: ‚ÄúSnap and Shop‚Äù. We believe that our new tool will give many advantages for customers and online business too. <br />\n",
        " <br />\n",
        "Specifically, for customers the ‚ÄúShop The Look‚Äù tool, will give a personalized user experience, with a fully automated process of matching products to scenes. Having an app which recognizes the clothes of a photo and search for similar products in online stores, will be more practical and time efficient. Secondly, by pointing out relevant alternatives based on what they are looking at, customers could be inspired and discover a much more bigger variety of similar outfits. Thirdly, many times users find they want, but they cant find where to find it online available. Giving that information instantly, will create more enjoyable customer journeys. <br />\n",
        "<br /> \n",
        "For online fashion businesses, the new tool will increase conversion and basket size. When shoppers are better engaged with their brand and its personalized style suggestions, they will probably also increase the frequency of purchases. Secondly, enabling consumers to buy the entire look in the picture with one click, removes the pain of complex human processes and their maintenance. So, companies could save time and costs, from every party that is involved in apparel purchases. Thirdly, the companies can leverage huge amounts of data from users and create detailed reports (e.g. sales performance, user queries) and create personalized landing pages or campaigns.\n",
        "\n",
        "***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4tmk54TLpMJ"
      },
      "outputs": [],
      "source": [
        "%%html \n",
        "<font size=\"+5\">\n",
        "<marquee style='width: 50%; color: orange;'><b>üëî üëì LET'S START  üëó üëú</b></marquee>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XypaI_ocLpMK"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnrfwsjrLpMK"
      },
      "source": [
        "<a class=\"anchor\" id=\"1\"></a> \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://i.ibb.co/1q50srZ/1.png\" alt=\"Explanatory Data Analysis\" style=\"width: 660px;\"/><br /> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wzXlJjWLpML"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '> Data Deep-Dive Analysis  <br /></b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dbv92tGLpML"
      },
      "source": [
        "* Importing the required libraries to run the necessary commands later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZqNp9ZDiHCm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import io\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glFTBHliSGt3"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5_FRJilde9t"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '> First things first<br /></b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FKkPOg-jnD1"
      },
      "source": [
        ">  üí°  In the ***Data_Acquisition.ipynb*** notebook provided seperately we have :\n",
        ">\n",
        "> 1.   Acquired the data from Skrout's Data Warehouse and saved them to a .csv format with the following columns:\n",
        ">  - product_id\n",
        ">  - image_url\n",
        ">  - description\n",
        ">  - brand\n",
        ">  - gender\n",
        ">  - category\n",
        ">  - category_name\n",
        "> 2. Downloaded the images from image_url and saved them locally, in the following sub-folders:\n",
        ">  - .. /women_topwear/\n",
        ">  - .. /women_bottomwear/\n",
        "> - .. /women_footwear/\n",
        ">  - .. /men_topwear/\n",
        ">  - .. /men_bottomwear/\n",
        ">  - .. /men_footwear/\n",
        "\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JREnOSMHh2O6"
      },
      "source": [
        ">  After compressing the images in Images.zip file , we have uploaded them in Google Drive. We have finaly created a new dataframe column named `path`, with the file path for each image. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4dlNYy0JVJJ"
      },
      "source": [
        "*  We now unzip the file skroutz_images.zip into Google Drive **( No need to run above cell , since the images are already unziped into Google Drive )** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JhZ1z7cLh1CF"
      },
      "outputs": [],
      "source": [
        "#!unzip /content/drive/MyDrive/shop_the_look/skroutz_images.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbTb525MLpMN"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '> Dataframe<br /></b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbpTWm2-J7S5"
      },
      "source": [
        "* The dataframe above contains the image paths allong with some description about the products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnXRBV9xlMzw"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/shop_the_look/skroutz_images.csv', usecols = ['product_id','description','brand','gender','category','category_name'])\n",
        "\n",
        "conditions = [\n",
        "    (df['gender'] == 'women') & (df['category'] == 'topwear'),\n",
        "    (df['gender'] == 'women') & (df['category'] == 'bottomwear'),\n",
        "    (df['gender'] == 'women') & (df['category'] == 'footwear'),\n",
        "    (df['gender'] == 'men') & (df['category'] == 'topwear'),\n",
        "    (df['gender'] == 'men') & (df['category'] == 'bottomwear'),\n",
        "    (df['gender'] == 'men') & (df['category'] == 'footwear')]\n",
        "\n",
        "choices = ['wtw', 'wbw', 'wft','mtw','mbw','mfw']\n",
        "\n",
        "df['code'] = np.select(conditions, choices)\n",
        "\n",
        "df['path'] = df['gender'] + '_' + df['category'] + '/' + df['code'] + '_' + df['product_id'].map(str) + '.jpeg'\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHZ3fKlDKHpa"
      },
      "source": [
        "* We have 9600 images in our database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e5GnhF_UKix"
      },
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll1xke2_LpMO"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '> Categories & Example</b> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzfkzVBfgq9g"
      },
      "source": [
        "> Below is an example of the product categories for men and women. We have 3 categories for each gender :\n",
        "- Topwear\n",
        "- Bottomwear\n",
        "- Footwear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_7HzyB9gmDV"
      },
      "outputs": [],
      "source": [
        "genders = list(df.gender.unique())\n",
        "categories = list(df.category.unique())\n",
        "\n",
        "plt.figure(figsize=(15,15), dpi=100)\n",
        "i = 1\n",
        "%cd /content/drive/MyDrive/shop_the_look\n",
        "for gender in genders:\n",
        "    for category in categories:\n",
        "            mdf = df[(df.gender == gender) & (df.category == category)]\n",
        "            img_url = mdf.path.iloc[4]\n",
        "            plt.subplot(6,3,i)\n",
        "            plt.imshow(io.imread(img_url))\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"{gender}s {category}\".title().replace(\"ns\", \"n's\"))\n",
        "            i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i6Ttdp6hFgt"
      },
      "source": [
        "* The number of products for Men and women almost equally distributed.\n",
        "5k products for Men and 4.6k products for Women."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaRSXubYLpMP"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Let's Plot</b> \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnZjKHMmdzVk"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10,7))\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "sns.countplot( x='gender', data=df, palette = \"Set3\").set(title='Products Split by Genders')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7vqSeKYhL-s"
      },
      "source": [
        "* Topwears are a majority in the dataset with 4k items whereas bottomwears are next up, with around 3.6k. Footwears are a minority in the dataset, having ~2k number of products."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3oCqqUbd1bc"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10,7))\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "plt.title('Distribution Across Fashion Categories', fontsize=20)\n",
        "sns.countplot( x='category', data=df, palette = \"coolwarm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZe9QaxMhZhE"
      },
      "source": [
        "* Here we can observe the distribution across product's categories in the dataset.\n",
        "Men's bottomwear have 1000 items each, whereas Women's and Men's topwear have 400 products each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KivwK5ITfMvp"
      },
      "outputs": [],
      "source": [
        "sns.set_theme(style=\"darkgrid\")\n",
        "plt.figure(figsize=(16,9))\n",
        "plt.xlabel(\"2\",fontsize=17)\n",
        "plt.ylabel(\"Product Categories\", fontsize=17)\n",
        "plt.title('Distribution Across Product Categories', fontsize=20)\n",
        "sns.countplot(palette =\"dark:salmon_r\", y=sorted(df.category_name, key=lambda x: df.category_name.value_counts()[x]), orient='v')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gofqH26hik0"
      },
      "source": [
        "* As far as the brands, we can see that Jack & Jones and Ralph Lauren are the most common brands in the dataset, with over 400 products each one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HwdaZ96gF6t"
      },
      "outputs": [],
      "source": [
        "print(\"25 Most Common Brands in the Dataset: \")\n",
        "df.brand.value_counts()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzK1LJeqLpMQ"
      },
      "source": [
        "***\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8ekDQFBWkz5"
      },
      "source": [
        "<a class=\"anchor\" id=\"2\"></a> \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://i.ibb.co/2j8HPqZ/21.png\" alt=\"Gender Classification\" style=\"width: 660px;\"/><br /> \n",
        "\n",
        "> <img src=\"https://i.ibb.co/cD7fRyR/train22.gif\" alt=\"Drawing\" style=\"width: 862px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz6uCXFYLpMQ"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Determining Person's  Gender</b> \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b086HR-rFkDj"
      },
      "source": [
        "> üí° Code Reference : \n",
        "\n",
        "*   https://www.kaggle.com/code/antoreepjana/gender-classification-using-resnet/notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1rohYsGYTVo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import resnet, ResNet50\n",
        "from tensorflow.keras import metrics, losses, optimizers, callbacks\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Flatten, Dropout\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG92eaS3LpMQ"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Dataset</b> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPxvvVcMXldQ"
      },
      "source": [
        "* üí° For this task we'll make use of the [Man/Woman classification dataset](https://www.kaggle.com/datasets/playlist/men-women-classification) from Kaggle. This is a manually collected and cleaned dataset containing 3354 pictures (jpg) of men (1414 files) and women (1940 files). After dowloading the data and uploading them to Google Drive, we unzip the file and we create a new folder named gender_classification_dataset that contains the training images for men and women.\n",
        "\n",
        "**( No need to run above cell , since the gender_classification_dataset is already uploaded into Google Drive )** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vATvZSiaXFk3"
      },
      "outputs": [],
      "source": [
        "#!cp \"/content/drive/MyDrive/shop_the_look/gender_classification_dataset.zip\" \"/content/\"\n",
        "#!unzip \"gender_classification_dataset.zip\"\n",
        "#!rm \"gender_classification_dataset.zip\"\n",
        "#!mv \"data/\" \"gender_classification_dataset/\"\n",
        "#!mv \"men/\" \"gender_classification_dataset/men/\"\n",
        "#!mv \"women/\" \"gender_classification_dataset/women/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pifmvnKuLpMR"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Tensorflow</b> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YmyHpR7ZLRf"
      },
      "source": [
        "* In the cell below we we'll confirm that we can connect to the GPU with tensorflow. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqODNEuVZKJG"
      },
      "outputs": [],
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvFovpXGLpMR"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Model Hyperparameters</b> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY58KNBRO401"
      },
      "source": [
        "* Next we define our model parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxgEmouBZebs"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "DROPOUT_RATE = 0.3\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 1e-3\n",
        "TARGET_SHAPE = (224,224,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIGAxGL4LpMR"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Training & Validation Sets</b> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCul7l39O_CF"
      },
      "source": [
        "* We split the dataset into training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S34EsmxaZgjf"
      },
      "outputs": [],
      "source": [
        "%cd  \"/content/drive/MyDrive/shop_the_look\"\n",
        "data_dir = 'gender_classification_dataset'\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(data_dir,validation_split=0.2,subset=\"training\",seed=123,image_size=TARGET_SHAPE[:2],batch_size=BATCH_SIZE)\n",
        "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(data_dir,validation_split=0.2,subset=\"validation\",seed=123,image_size=TARGET_SHAPE[:2],batch_size=BATCH_SIZE)\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1 / 255.0)\n",
        "test_generator = test_datagen.flow_from_directory(directory=data_dir,target_size=TARGET_SHAPE[:2],color_mode=\"rgb\",batch_size=1, class_mode=None,shuffle=False,seed=42)\n",
        "class_names = train_dataset.class_names"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = list(test_generator.class_indices.keys())\n",
        "class_names"
      ],
      "metadata": {
        "id": "H7-I4EAGOXNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bisRTVJ3LpMR"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Subset Plotting</b> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77JOsEzUPD4S"
      },
      "source": [
        "* We plot subset of the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-JfHrM2Zzoa"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_dataset.take(1):\n",
        "    for i in range(9):\n",
        "      ax = plt.subplot(3, 3, i + 1)\n",
        "      plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "      plt.title(class_names[labels[i]])\n",
        "      plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPszwPJJLpMS"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>\"Better performance with the tf.data API\"</b> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjB6Ywy1Rcci"
      },
      "source": [
        "* We use the tf.data API since, for achieving peak performance the model requires an efficient input pipeline that delivers data for the next step before the current step has finished"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZPR_8DTcJtN"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "cbgIVtQkOeBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_0HW9RvLpMS"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>ResNET50</b> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrT7BEwTR7Fd"
      },
      "source": [
        "* In the next cell we will built a ResNET50 backbone initialized with 'ImageNet' weights attached to a fully connected network that outputs the probability scores for the two classes\n",
        "* ResNet50 is a variant of ResNet model which has 48 Convolution layers along with 1 MaxPool and 1 Average Pool layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shYF3lTDcOCW"
      },
      "outputs": [],
      "source": [
        "base_model = ResNet50(weights=\"imagenet\", input_shape=TARGET_SHAPE, include_top=False)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(512, activation=\"relu\")(x)\n",
        "x = Dropout(DROPOUT_RATE)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(128, activation=\"relu\")(x)\n",
        "x = Dropout(DROPOUT_RATE)(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "output = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "gender_classifier = Model(base_model.input, output, name=\"Gender_Classifier\")\n",
        "gender_classifier.compile(optimizer=optimizers.SGD(LEARNING_RATE), loss=losses.BinaryCrossentropy(), metrics=[\"accuracy\"])\n",
        "gender_classifier.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yObact85LpMS"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Fit The Model - ModelCheckpoint & EarlyStopping</b> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCsabGcdSF-l"
      },
      "source": [
        "* We fit the model using ModelCheckpoint to save the weights at some interval, and using EarlyStopping, so to stop training when the val_accuracy metric has stopped improving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdXePmuGcTRb"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = callbacks.ModelCheckpoint(\"gender_classification_model.h5\", monitor=\"val_accuracy\", mode=\"max\", save_best_only=True)\n",
        "earlystopping = callbacks.EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=10)\n",
        "\n",
        "history = gender_classifier.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS, callbacks=[model_checkpoint, earlystopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaCbeleDssCU"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(gender_classifier, show_layer_names=True, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqgj8hx7ssnA"
      },
      "outputs": [],
      "source": [
        "training_accuracy_resnet      = history.history['accuracy'][-1]\n",
        "training_loss_resnet          = history.history['loss'][-1]\n",
        "validation_accuracy_resnet    = history.history['val_accuracy'][-1]\n",
        "validation_loss_resnet        = history.history['val_loss'][-1]\n",
        "print(\"Training Accuracy ResNet   :\", training_accuracy_resnet )\n",
        "print(\"Training Loss ResNet       :\", training_loss_resnet)\n",
        "print(\"Validation Accuracy ResNet :\", validation_accuracy_resnet)\n",
        "print(\"Validation Loss ResNet     :\", validation_loss_resnet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOKZZ1Sqsr0t"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "y_vloss = history.history['val_loss']\n",
        "y_loss = history.history['loss']\n",
        "x_len = numpy.arange(len(y_loss))\n",
        "plt.figure(figsize=(16,9))\n",
        "plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n",
        "plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP5Ljo0gs7JM"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[16,9])\n",
        "plt.plot(history.history['accuracy'],'r',linewidth=3.0)\n",
        "plt.plot(history.history['val_accuracy'],'b',linewidth=3.0)\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
        "plt.xlabel('Epochs ',fontsize=16)\n",
        "plt.ylabel('Accuracy',fontsize=16)\n",
        "plt.title('Accuracy Curves',fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMBeP9LsLpMS"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Evaluating The Model</b> \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = gender_classifier.predict(test_generator)\n",
        "y_test = test_generator.classes"
      ],
      "metadata": {
        "id": "dvNGfALO_gsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for evaluation metrics precision, recall, f1 etc\n",
        "def modelEvaluation(predictions, y_test_set, model_name, label):\n",
        "    # Print model evaluation to predicted result\n",
        "    print(\"==========\",model_name,\"==========\")\n",
        "    print (\"\\nAccuracy on validation set: {:.4f}\".format(accuracy_score(y_test_set, predictions)))\n",
        "    print (\"\\nClassification report : \\n\", classification_report(y_test_set, predictions, target_names=label))\n",
        "    print (\"\\nConfusion Matrix : \\n\", confusion_matrix(y_test_set, predictions))\n",
        "    plt.figure(figsize=(10,10))\n",
        "    y_pred = ['men' if i==0 else 'women' for i in predictions]\n",
        "    y_test = ['men' if i==0 else 'women' for i in y_test_set]\n",
        "    sns.heatmap(confusion_matrix(y_test, y_pred, labels=label), annot=True, fmt='g',yticklabels=label, xticklabels=label, cmap='viridis')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "LGNK4S4jM9CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelEvaluation(predictions, y_test, \"Gender Classifier\", label=class_names)"
      ],
      "metadata": {
        "id": "kI4XJ4-CM-Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsvakGQMt8Q6"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Prediction using Test Generator</b> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M73HN44zvOA"
      },
      "outputs": [],
      "source": [
        "files=test_generator.filenames\n",
        "class_dict=test_generator.class_indices # a dictionary of the form class name: class index\n",
        "rev_dict={}\n",
        "for key, value in class_dict.items():\n",
        "    rev_dict[value]=key   # dictionary of the form class index: class name\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2, os\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "for i, p in enumerate(predictions[:9]):\n",
        "    plt.subplot(3,3, i+1)\n",
        "    \n",
        "    if p[0] <= 0.5:\n",
        "      plt.title('Actual:' + files[i].split('/')[0] + ' | Pred: men')\n",
        "    else:\n",
        "      plt.title('Actual:' + files[i].split('/')[0] + ' | Pred: women')\n",
        "\n",
        "    img_path = os.path.join(data_dir, files[i])\n",
        "    image = cv2.imread(img_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(image)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KNrHhYSMOvpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ1gYyRiLpMT"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Saving -> Google Drive</b> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJSEcTCESl80"
      },
      "source": [
        "* We finally save the model weights to Google Drive for further use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtXwr3xNdyJ0"
      },
      "outputs": [],
      "source": [
        "!cp gender_classification_model.h5 /content/drive/MyDrive/shop_the_look/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKmF-jNPLpMT"
      },
      "source": [
        "***\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lim5pq8XLpMT"
      },
      "source": [
        "<a class=\"anchor\" id=\"3\"></a> \n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://i.ibb.co/zFXwwcH/3.png\" alt=\"Object Localization\" style=\"width: 660px;\"/><br /> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj0rLrLJGeck"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Using Yolo v5 </b> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZRzQ6fyB1nb"
      },
      "source": [
        "> üí° Code Reference : \n",
        "\n",
        "*   https://colab.research.google.com/github/roboflow-ai/yolov5-custom-training-tutorial/blob/main/yolov5-custom-training.ipynb\n",
        "*   https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPc1pTPzSudK"
      },
      "source": [
        "> We'll use YOLO object detection for detecting fashion products in three categories - top wear, bottom wear, and footwear. We will first clone the yolov5 github repository and install the dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhonmSM2GkKx"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt # install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVNMWuTgTtUz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CPdFRVgT0mi"
      },
      "source": [
        "* We check if GPU is running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03sfFRLwbAsZ"
      },
      "outputs": [],
      "source": [
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WkLAwB2LpMU"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '> Roboflow Dataset </b> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_QNRZlOT-Io"
      },
      "source": [
        "* We will train the yolov5 model using an published Roboflow dataset that has topwear, bottomwear, and footwear annoted images. We first should install and import Roboflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9yCsd5vIVkg"
      },
      "outputs": [],
      "source": [
        "!pip install roboflow\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(model_format=\"yolov5\", notebook=\"ultralytics\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87m4exTbIW8K"
      },
      "outputs": [],
      "source": [
        "# set up environment\n",
        "os.environ[\"DATASET_DIRECTORY\"] = \"/content/datasets\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6-PgFGwVX8V"
      },
      "source": [
        "* To download and unzip the dataset in yolov5 format from Roboflow, we should run the above cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WQKJVafJqO9"
      },
      "outputs": [],
      "source": [
        "rf = Roboflow(api_key=\"r6rpj5oyHf5bQdYsqukZ\")\n",
        "project = rf.workspace(\"new-workspace-w4orl\").project(\"fashion_obj_detection\")\n",
        "dataset = project.version(1).download(\"yolov5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIx3rK2gLpMU"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '> Training the Model </b> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0dRm_zaVr6K"
      },
      "source": [
        "* We train the model by running yolov5's train.py. It will download the weights and kick-off training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPW3AW5ZLoUa"
      },
      "outputs": [],
      "source": [
        "!python train.py --img 640 --batch 2 --epochs 150 --data {dataset.location}/data.yaml --weights /content/yolov5/weights/yolov5s.pt --nosave --cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5JN_o8SWiLg"
      },
      "source": [
        "* Next up we will visualize the training results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zISmfnt0QDg9"
      },
      "outputs": [],
      "source": [
        "# Start tensorboard\n",
        "# Launch after you have started training\n",
        "# logs save in the folder \"runs\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxU1gQYbLpMV"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '> Model Weights </b> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqQuIohFXada"
      },
      "source": [
        "* Finally we export our model's weights for future use and rename the file to 'object_detection.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCecAEUITG9U"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('./runs/train/exp2/weights/last.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6Cfg05XYgR5"
      },
      "source": [
        "* With detect.py we will run our saved weights on our test images and we will be able to visualize them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V16lQOR4XhLv"
      },
      "outputs": [],
      "source": [
        "%cd yolov5\n",
        "!python detect.py --weights /content/drive/MyDrive/shop_the_look/object_detection.pt --img 416 --conf 0.1 --source /content/drive/MyDrive/shop_the_look/test_image_1.jpg\n",
        "Image(filename='/content/drive/MyDrive/shop_the_look/yolov5/runs/detect/exp/test_image_1.jpg', width=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzF-WEQjLpMV"
      },
      "source": [
        "***\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGBr4rDWLpMV"
      },
      "source": [
        "<a class=\"anchor\" id=\"4\"></a> \n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://i.ibb.co/n0Pb0q5/4.png\" alt=\"Object Localization\" style=\"width: 660px;\"/><br /> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jySjP9NLpMV"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '> Extract different clothing items from an image </b> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNTsWgYpGgoW"
      },
      "source": [
        "> üí° Code Reference :\n",
        "\n",
        "*   https://www.youtube.com/watch?v=VmqESKaaYVk&ab_channel=DeepLearning\n",
        "*   https://github.com/biplob004/codeShare/blob/main/detect.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oV8C3OeY_-V"
      },
      "source": [
        "> Now we will build a function to detect the topwear, bottomwear and footwear from the input image, and it will output one image for each class detected. First we load the object localization model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC-i9pPlUJyH"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/shop_the_look/yolov5\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "%matplotlib inline\n",
        "import torch\n",
        "model = torch.hub.load('.', 'custom', path='/content/drive/MyDrive/shop_the_look/object_detection.pt', source='local') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYPcfV9zLpMV"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '> 2 x Functions </b> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWHePjmXZd7-"
      },
      "source": [
        "* The `extract_clothes` function takes as input the path to the image file that we want to extract clothes from and outputs a dictionary containing detected Object Class as Keys and the image slice as Values. In case there are multiple items of the same class, the item with highest confidence score is returned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqyTTGN4aEwB"
      },
      "source": [
        "* The `plot_clothes` function takes as input the dictionary from `extract_clothes` output and returns an matplotlib.figure object with the detected images in a row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pF5vvnnaX_7a"
      },
      "outputs": [],
      "source": [
        "def extract_clothes(img_path: str): \n",
        "    image = plt.imread(img_path)\n",
        "    results = model(image_path)\n",
        "    topwear_score, bottomwear_score, footwear_score = 0.5, 0.5, 0.5\n",
        "    outputs = {\"original_image\": image}\n",
        "    for index, row in results.pandas().xyxy[0].iterrows():\n",
        "      XMIN = int(row['xmin'])\n",
        "      YMIN = int(row['ymin']) \n",
        "      XMAX = int(row['xmax'])\n",
        "      YMAX = int(row['ymax'])\n",
        "      if (row['class'] == 0) and (row['confidence'] > topwear_score) :\n",
        "          outputs[\"topwear\"] = image[YMIN:YMAX, XMIN:XMAX]\n",
        "      elif (row['class'] == 1) and (row['confidence'] > bottomwear_score):\n",
        "          outputs[\"bottomwear\"] = image[YMIN:YMAX, XMIN:XMAX]\n",
        "      elif (row['class'] == 2) and (row['confidence'] > footwear_score):\n",
        "          outputs[\"footwear\"] = image[YMIN:YMAX, XMIN:XMAX]\n",
        "    return outputs\n",
        "\n",
        "def plot_clothes(**images):\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "      plt.subplot(1, n, i + 1)\n",
        "      plt.axis(\"off\")\n",
        "      plt.title(' '.join(name.split('_')).title())\n",
        "      plt.imshow(image)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbg9i63zLpMW"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '> | Example A </b> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUXWv7N-ZipK"
      },
      "source": [
        "* Lets see 2 examples below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcYTm0-LI4D0"
      },
      "outputs": [],
      "source": [
        "image_path = '/content/drive/MyDrive/shop_the_look/' + df.path.iloc[96]\n",
        "outputs = extract_clothes(image_path)\n",
        "plot_clothes(**outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUwh73eELpMW"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '> | Example B </b> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxaYMy0MY9my"
      },
      "outputs": [],
      "source": [
        "image_path = '/content/drive/MyDrive/shop_the_look/' + df.path.iloc[156]\n",
        "outputs = extract_clothes(image_path)\n",
        "plot_clothes(**outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg9HPijPLpMW"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '> Runtime Results </b> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1O3cnXIaFnN"
      },
      "source": [
        " * The runtime speed of the pipeline above is on average 62.4 ms "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wW7leUUBZ9JI"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "outputs = extract_clothes(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIiKGK8uLpMW"
      },
      "source": [
        "***\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiAMFsmnLpMX"
      },
      "source": [
        "<a class=\"anchor\" id=\"5\"></a> \n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://i.ibb.co/Nn5m3JW/5.png\" alt=\"Embeddings Generation\" style=\"width: 860px;\"/><br /> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP5xTAx6a1Mr"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '> Dataset: \"Shop The Look\" </b> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuN-rcTwCLzv"
      },
      "source": [
        "> üí° Code Reference: \n",
        "\n",
        "\n",
        "1.   https://github.com/kang205/STL-Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyBqBWMabZ1y"
      },
      "source": [
        "* We use embedding generation to represent images/products such that similar ones are grouped together whereas dissimilar ones are moved away so that in order to retrieve products that are similar to the products present in the query image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWMIa_zOa7Hy"
      },
      "outputs": [],
      "source": [
        "import os, cv2, shutil, random\n",
        "import json\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from skimage import io\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3-H3M9-bhYr"
      },
      "source": [
        "* In order to convert images to n-dimensional vector representations of the images, we'll use a special type of Neural Network called Siamese Network. This network which takes in three inputs - anchor, positive and negative, such that anchor, and positive inputs are similar whereas anchor and negative inputs are dissimilar. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4mFTjLybm5L"
      },
      "source": [
        "* \"Shop The Look\" is a dataset which provides with scene and product pairs. Product is an image of a product in professional setting whereas a scene is the image of the same product but in casual or non-professional setting. We can easily create anchor and positive pairs from this dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZibPcZuLpMX"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '> Clone Repo </b> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7kgNBcCb77z"
      },
      "source": [
        "* We first clone the shop_the_look github repo in order to dowload the data.  https://github.com/kang205/STL-Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jueA7nNjLlW"
      },
      "outputs": [],
      "source": [
        "%cd  \"/content/drive/MyDrive/shop_the_look\"\n",
        "!git clone https://github.com/kang205/STL-Dataset  # clone repo\n",
        "%cd STL-Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYnfCb3teBUI"
      },
      "outputs": [],
      "source": [
        "stl_dataset = \"/content/drive/MyDrive/shop_the_look/STL-Dataset\"\n",
        "files = [os.path.join(stl_dataset, x) for x in os.listdir(stl_dataset) if (x.endswith(\"fashion-cat.json\") | x.endswith(\"fashion.json\"))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOZ13lIbd-t2"
      },
      "outputs": [],
      "source": [
        "with open(files[0], \"r\") as f:\n",
        "  categories = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4ZM_wtOLpMX"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>2 x Functions </b> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ajEjhM8caoF"
      },
      "source": [
        "* `convert_to_url` function takes as input the signature of the image and returns the url. `categorize` function categorizes the products into 3 broad categories - Topwear, Bottomwear and Footwear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_cDrvCBcTXl"
      },
      "outputs": [],
      "source": [
        "def convert_to_url(signature):\n",
        "    prefix = 'http://i.pinimg.com/400x/%s/%s/%s/%s.jpg'\n",
        "    return prefix % (signature[0:2], signature[2:4], signature[4:6], signature)\n",
        "\n",
        "def categorize(x):\n",
        "    if x == 'Shoes':\n",
        "        return 'footwear'\n",
        "    elif x in ['Clothing|Pants', 'Clothing|Shorts', 'Clothing|Skirts']:\n",
        "        return 'bottomwear'\n",
        "    else:\n",
        "        return 'topwear'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjNc5J7ELpMY"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '> Dataframe </b> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixES6S9tdT3k"
      },
      "source": [
        "* We read the data and convert them to a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb-hFJABc7Jl"
      },
      "outputs": [],
      "source": [
        "data = [json.loads(line) for line in open(files[1], 'r')]\n",
        "data = pd.DataFrame(data)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuH8faLVLpMY"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '> Replacing Signatures & Scene Columns </b> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bZ3T_Uzdgdj"
      },
      "source": [
        "* We replace the signatures in product and scene colums with the url, and we categorize each product based on the function mentioned above. We finally split the bounding box coordinates into 4 different columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejaUlLNCd5am"
      },
      "outputs": [],
      "source": [
        "data = [json.loads(line) for line in open(files[1], 'r')]\n",
        "data = pd.DataFrame(data)\n",
        "data['category'] = data['product'].apply(lambda x: categories[x].replace('Apparel & Accessories|', ''))\n",
        "data = data[data['category'].apply(lambda x: True if ('Clothing' in x) or ('Shoes' in x) else False)]\n",
        "data = data[data['category'].apply(lambda x: True if ('Sunglasses' not in x) else False)]\n",
        "data['category'] = data['category'].apply(lambda x: categorize(x))\n",
        "data['product'] = data['product'].apply(lambda x: convert_to_url(x))\n",
        "data['scene'] = data['scene'].apply(lambda x: convert_to_url(x))\n",
        "data['XMIN'] = data['bbox'].apply(lambda x: x[0])\n",
        "data['YMIN'] = data['bbox'].apply(lambda x: x[1])\n",
        "data['XMAX'] = data['bbox'].apply(lambda x: x[2])\n",
        "data['YMAX'] = data['bbox'].apply(lambda x: x[3])\n",
        "data = data.drop([\"bbox\"], axis=1)\n",
        "print(\"Total Pairs\", data.shape[0])\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg-yvUoGeAek"
      },
      "source": [
        "* We have 31249 Unique Products and 25799 Unique Scenes, meaning that different products can appear in the same scene."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFv-i6hVmFFE"
      },
      "outputs": [],
      "source": [
        "print(\"Number of Unique Products:\\t\", data[\"product\"].nunique())\n",
        "print(\"Number of Unique Scenes:\\t\", data[\"scene\"].nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpIPnT-NLpMZ"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '> Product Categories Distribution </b> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY5lNaXJeV4u"
      },
      "source": [
        "* The product categories are distributed as following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx-teKfBmGrw"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data.category)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJIBqV3fmIkh"
      },
      "outputs": [],
      "source": [
        "topwear = data[data['category'] == 'topwear'].reset_index(drop=True)\n",
        "bottomwear = data[data['category'] == 'bottomwear'].reset_index(drop=True)\n",
        "footwear = data[data['category'] == 'footwear'].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yPtg55FLpMZ"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>3 x Functions </b> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti8RRQ8uecTR"
      },
      "source": [
        "* The `download_images` function downloads the images by using the URL and saves them locally. `create_negative` function selects a random image from the same category that will act as a negative sample. `plot_samples` function plots Anchor, Positive and Negative images in a row for a given batch of images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnP24aCkmKmw"
      },
      "outputs": [],
      "source": [
        "def download_images(dataframe, target_folder):  \n",
        "    embeddings_dir = \"embeddings_train_data\"\n",
        "    data = list()\n",
        "    if not os.path.exists(embeddings_dir):\n",
        "        os.mkdir(embeddings_dir)\n",
        "    for i, row in tqdm(dataframe.iterrows(), total=dataframe.shape[0]):\n",
        "        try:\n",
        "            product_url, product_name = row[\"product\"], row[\"product\"].split(\"/\")[-1]\n",
        "            scene_url, scene_name = row[\"scene\"], row[\"scene\"].split(\"/\")[-1]\n",
        "            target_dir = os.path.join(embeddings_dir, target_folder)\n",
        "            if not os.path.exists(target_dir):\n",
        "                os.mkdir(target_dir)\n",
        "                os.mkdir(os.path.join(target_dir, \"anchor\"))\n",
        "                os.mkdir(os.path.join(target_dir, \"positive\"))\n",
        "            if not os.path.isfile(os.path.join(target_dir, \"anchor\", product_name)):\n",
        "                product = Image.fromarray(io.imread(product_url))\n",
        "                product.save(os.path.join(target_dir, \"anchor\", product_name))\n",
        "            if not os.path.isfile(os.path.join(target_dir, \"positive\", scene_name)):\n",
        "                scene = io.imread(scene_url)\n",
        "                XMIN, YMIN, XMAX, YMAX = int(scene.shape[1] * row['XMIN']), int(scene.shape[0] * row['YMIN']), int(scene.shape[1] * row['XMAX']), int(scene.shape[0] * row['YMAX']) \n",
        "                scene = Image.fromarray(scene[YMIN:YMAX, XMIN:XMAX])\n",
        "                scene.save(os.path.join(target_dir, \"positive\", scene_name))\n",
        "            data.append((os.path.join(target_dir, \"anchor\", product_name), os.path.join(target_dir, \"positive\", scene_name)))\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            continue\n",
        "    return data\n",
        "\n",
        "def create_negative(List, positive):\n",
        "    negative = random.choice(List)\n",
        "    if negative != positive:\n",
        "        return negative\n",
        "    else:\n",
        "        create_negative(List, positive)\n",
        "        \n",
        "def plot_samples(dataframe, size=4, random_state=4):\n",
        "    for _, row in dataframe.sample(n=size, random_state=random_state).iterrows():\n",
        "        anchor, positive, negative = row[\"anchor\"], row[\"positive\"], row[\"negative\"] \n",
        "        anchor, positive, negative = io.imread(anchor), io.imread(positive), io.imread(negative)\n",
        "        plt.figure(figsize=(16,9))\n",
        "        plt.subplot(131)\n",
        "        plt.imshow(anchor)\n",
        "        plt.axis(\"off\"); plt.title(\"Anchor\")\n",
        "        plt.subplot(132)\n",
        "        plt.imshow(positive)\n",
        "        plt.axis(\"off\"); plt.title(\"Positive\")\n",
        "        plt.subplot(133)\n",
        "        plt.imshow(negative)\n",
        "        plt.axis(\"off\"); plt.title(\"Negative\")\n",
        "        plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWuezJTJLpMZ"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Download & Save  </b> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9LzBwcDfomo"
      },
      "source": [
        "* We download and save the images in /content/drive/MyDrive/shop_the_look/STL-Dataset/embeddings_train_data for Topwear, Bottomwear, and Footwear in separate folders, and we finally create a csv with anchor, positive and negative image paths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KG-MnTIfBTR"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '> Topwear  </b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHaQTV70mL30"
      },
      "outputs": [],
      "source": [
        "top_pairs = download_images(topwear, \"topwear\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64b8FpCKmQht"
      },
      "outputs": [],
      "source": [
        "top_pairs_csv = pd.DataFrame(top_pairs, columns=[\"anchor\", \"positive\"])\n",
        "top_pairs_csv[\"negative\"] = top_pairs_csv[\"positive\"].apply(lambda x: create_negative(top_pairs_csv.positive.to_list(), x)) \n",
        "print(\"Total Pairs: \", top_pairs_csv.shape[0])\n",
        "top_pairs_csv.to_csv(\"top_pairs.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryDN6-b6zy9o"
      },
      "outputs": [],
      "source": [
        "plot_samples(top_pairs_csv, size=3, random_state=31)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1n-CBd2LpMa"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Bottomwear </b> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbxP0tuzfLnJ"
      },
      "outputs": [],
      "source": [
        "bottom_pairs = download_images(bottomwear, \"bottomwear\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZc0DL0Wxm3Y"
      },
      "outputs": [],
      "source": [
        "bottom_pairs_csv = pd.DataFrame(bottom_pairs, columns=[\"anchor\", \"positive\"])\n",
        "bottom_pairs_csv[\"negative\"] = bottom_pairs_csv[\"positive\"].apply(lambda x: create_negative(bottom_pairs_csv.positive.to_list(), x))\n",
        "print(\"Total Pairs: \", bottom_pairs_csv.shape[0])\n",
        "bottom_pairs_csv.to_csv(\"bottom_pairs.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjpJ5Q-xxpel"
      },
      "outputs": [],
      "source": [
        "plot_samples(bottom_pairs_csv, size=3, random_state=99)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Oz5Z0SYy_BS"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Footwear </b> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0rJVp6lzCGf"
      },
      "outputs": [],
      "source": [
        "foot_pairs = download_images(footwear, \"footwear\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb6pKJpNGqov"
      },
      "outputs": [],
      "source": [
        "foot_pairs_csv = pd.DataFrame(foot_pairs, columns=[\"anchor\", \"positive\"])\n",
        "foot_pairs_csv[\"negative\"] = foot_pairs_csv[\"positive\"].apply(lambda x: create_negative(foot_pairs_csv.positive.to_list(), x))\n",
        "print(\"Total Pairs: \", foot_pairs_csv.shape[0])\n",
        "foot_pairs_csv.to_csv(\"foot_pairs.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nrXpIHtGtGK"
      },
      "outputs": [],
      "source": [
        "plot_samples(foot_pairs_csv, size=3, random_state=99)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOSYo7P7HB-M"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Embeddings-Generation </b> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfzBYvbmDXgc"
      },
      "source": [
        "> üí° Code Reference:\n",
        "\n",
        "*   https://keras.io/examples/vision/siamese_network/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW70WcOnLpMb"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Siamese Network </b> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWrfOd2Qgo05"
      },
      "source": [
        "* Now it is time to train our Siamese network to represent images/products such that similar ones are grouped together whereas dissimilar ones are moved away"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BaC47iaHJ0F"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout, Input\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.applications import resnet, resnet_v2, ResNet50, ResNet50V2, EfficientNetB3, InceptionV3, InceptionResNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhHpXum0JICI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "embeddings_dir = \"embeddings_train_data\"\n",
        "if not os.path.isdir(embeddings_dir):    \n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !cp \"/content/drive/MyDrive/shop_the_look/STL-Dataset/embeddings_train_data\" \"/content/\"\n",
        "    !cp \"/content/drive/MyDrive/shop_the_look/STL-Dataset/top_pairs.csv\" \"/content/\"\n",
        "    !cp \"/content/drive/MyDrive/shop_the_look/STL-Dataset/bottom_pairs.csv\" \"/content/\"\n",
        "    !cp \"/content/drive/MyDrive/shop_the_look/STL-Dataset/foot_pairs.csv\" \"/content/\"\n",
        "    print(\"Data Loaded Successfully!\")\n",
        "else:\n",
        "    print(\"Data already loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huf4l5eBJkDE"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(filename):\n",
        "    \"\"\"\n",
        "    Load the specified file as a JPEG image, preprocess it and\n",
        "    resize it to the target shape.\n",
        "    \"\"\"\n",
        "\n",
        "    image_string = tf.io.read_file(filename)\n",
        "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, TARGET_SHAPE[:2])\n",
        "    return image\n",
        "\n",
        "\n",
        "def preprocess_triplets(anchor, positive, negative):\n",
        "    \"\"\"\n",
        "    Given the filenames corresponding to the three images,\n",
        "    preprocess them.\n",
        "    \"\"\"\n",
        "    return preprocess_image(anchor), preprocess_image(positive), preprocess_image(negative)\n",
        "\n",
        "def visualize(anchor, positive, negative, n=3):\n",
        "    \"\"\"Visualize triplets from the supplied batches.\"\"\"\n",
        "\n",
        "    def show(ax, image):\n",
        "        ax.imshow(image)\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    fig = plt.figure(figsize=(3*n, 9))\n",
        "\n",
        "    axs = fig.subplots(n, 3)\n",
        "    for i in range(n):\n",
        "        show(axs[i, 0], anchor[i])\n",
        "        show(axs[i, 1], positive[i])\n",
        "        show(axs[i, 2], negative[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V060FzHDJvUm"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-4\n",
        "TARGET_SHAPE = (224, 224, 3)\n",
        "EMBEDDING_DIMENSION = 256\n",
        "EPOCHS = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knRm89KMJxjV"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "base_model = ResNet50(weights=\"imagenet\", input_shape=TARGET_SHAPE, include_top=False)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(512, activation=\"relu\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(256, activation=\"relu\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "output = Dense(EMBEDDING_DIMENSION, activation=\"linear\")(x)\n",
        "\n",
        "embedding = Model(base_model.input, output, name=\"Embedding\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q55CYuFkJztn"
      },
      "outputs": [],
      "source": [
        "class DistanceLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    This layer is responsible for computing the distance between the anchor\n",
        "    embedding and the positive embedding, and the anchor embedding and the\n",
        "    negative embedding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, anchor, positive, negative):\n",
        "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
        "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
        "        return (ap_distance, an_distance)\n",
        "\n",
        "anchor_input = Input(name=\"anchor\", shape=TARGET_SHAPE)\n",
        "positive_input = Input(name=\"positive\", shape=TARGET_SHAPE)\n",
        "negative_input = Input(name=\"negative\", shape=TARGET_SHAPE)\n",
        "\n",
        "distances = DistanceLayer()(embedding(resnet.preprocess_input(anchor_input)), embedding(resnet.preprocess_input(positive_input)), embedding(resnet.preprocess_input(negative_input)))\n",
        "\n",
        "siamese_network = Model(inputs=[anchor_input, positive_input, negative_input], outputs=distances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbgSrVQmJ6M0"
      },
      "outputs": [],
      "source": [
        "class SiameseModel(Model):\n",
        "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
        "\n",
        "    Computes the triplet loss using the three embeddings produced by the\n",
        "    Siamese Network.\n",
        "\n",
        "    The triplet loss is defined as:\n",
        "       L(A, P, N) = max(‚Äñf(A) - f(P)‚Äñ¬≤ - ‚Äñf(A) - f(N)‚Äñ¬≤ + margin, 0)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, siamese_network, margin=0.5):\n",
        "        super(SiameseModel, self).__init__()\n",
        "        self.siamese_network = siamese_network\n",
        "        self.margin = margin\n",
        "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.siamese_network(inputs)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self._compute_loss(data)\n",
        "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.siamese_network.trainable_weights))\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        loss = self._compute_loss(data)\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def _compute_loss(self, data):\n",
        "        ap_distance, an_distance = self.siamese_network(data)\n",
        "        loss = ap_distance - an_distance\n",
        "        loss = tf.maximum(loss + self.margin, 0.0)\n",
        "        return loss\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqcgzfUri9JD"
      },
      "source": [
        "* We will fit the Siamese Network for each one of the topwear, botomwear and footwear, and save the embeddings in topwear_embedding.h5, bottomwear_embedding.h5, and footwear_embedding.h5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do7vlpxSLpMd"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Train-Topwear </b> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKJyjo9BJ8f4"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/shop_the_look/STL-Dataset/\n",
        "dataframe = pd.read_csv(\"top_pairs.csv\")\n",
        "dataframe.negative.fillna(dataframe.negative.mode()[0], inplace=True)\n",
        "for column in dataframe.columns:\n",
        "    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"\\\\\",\"/\")) \n",
        "\n",
        "anchor_images = dataframe.anchor.to_list()\n",
        "positive_images = dataframe.positive.to_list()\n",
        "negative_images = dataframe.negative.to_list()\n",
        "image_count = len(anchor_images)\n",
        "\n",
        "anchor_dataset = tf.data.Dataset.from_tensor_slices(anchor_images)\n",
        "positive_dataset = tf.data.Dataset.from_tensor_slices(positive_images)\n",
        "negative_dataset = tf.data.Dataset.from_tensor_slices(negative_images)\n",
        "\n",
        "dataset = tf.data.Dataset.zip((anchor_dataset, positive_dataset, negative_dataset))\n",
        "dataset = dataset.shuffle(buffer_size=1024)\n",
        "dataset = dataset.map(preprocess_triplets)\n",
        "\n",
        "train_dataset = dataset.take(round(image_count * 0.8))\n",
        "val_dataset = dataset.skip(round(image_count * 0.8))\n",
        "\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
        "train_dataset = train_dataset.prefetch(8)\n",
        "\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
        "val_dataset = val_dataset.prefetch(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMU9_mxHKDNv"
      },
      "outputs": [],
      "source": [
        "visualize(*list(train_dataset.take(7).as_numpy_iterator())[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jXSgNxgKgyy"
      },
      "outputs": [],
      "source": [
        "callbacks = [EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xgkbFjTKn45"
      },
      "outputs": [],
      "source": [
        "siamese_model = SiameseModel(siamese_network)\n",
        "siamese_model.compile(optimizer=optimizers.Adam(LEARNING_RATE))\n",
        "siamese_model.fit(train_dataset, epochs=EPOCHS, validation_data=val_dataset, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyVMvZPmr974"
      },
      "outputs": [],
      "source": [
        "sample = next(iter(train_dataset))\n",
        "anchor, positive, negative = sample\n",
        "anchor_embedding, positive_embedding, negative_embedding = (embedding(resnet.preprocess_input(anchor)), embedding(resnet.preprocess_input(positive)), embedding(resnet.preprocess_input(negative)),)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnsKOy9msp9p"
      },
      "outputs": [],
      "source": [
        "anchors, positives, negatives = sample\n",
        "for i in range(len(anchor_embedding)):\n",
        "    anchor, positive, negative = anchors[i], positives[i], negatives[i]\n",
        "    positive_distance = np.linalg.norm(anchor_embedding[i] - positive_embedding[i], ord=2)*1e4\n",
        "    negative_distance = np.linalg.norm(anchor_embedding[i] - negative_embedding[i], ord=2)*1e4\n",
        "    positive_distance, negative_distance = round(positive_distance)/1e4, round(negative_distance)/1e4\n",
        "    plt.figure(figsize=(16,9))\n",
        "    plt.subplot(131)\n",
        "    plt.imshow(anchor)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Anchor\")\n",
        "    plt.subplot(132)\n",
        "    plt.imshow(positive)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Anchor to Positive Distance: {positive_distance}\")\n",
        "    plt.subplot(133)\n",
        "    plt.imshow(negative)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Anchor to Negative Distance: {negative_distance}\")\n",
        "    plt.show()        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5auR7HxtoCs"
      },
      "outputs": [],
      "source": [
        "embedding.save(\"topwear_embedding.h5\")\n",
        "!cp topwear_embedding.h5 /content/drive/MyDrive/shop_the_look/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m50Fl_1tArR"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Train-Bottomwear </b> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJGZGXartEMl"
      },
      "outputs": [],
      "source": [
        "dataframe = pd.read_csv(\"bottom_pairs.csv\")\n",
        "dataframe.negative.fillna(dataframe.negative.mode()[0], inplace=True)\n",
        "for column in dataframe.columns:\n",
        "    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"\\\\\",\"/\")) \n",
        "\n",
        "anchor_images = dataframe.anchor.to_list()\n",
        "positive_images = dataframe.positive.to_list()\n",
        "negative_images = dataframe.negative.to_list()\n",
        "image_count = len(anchor_images)\n",
        "\n",
        "anchor_dataset = tf.data.Dataset.from_tensor_slices(anchor_images)\n",
        "positive_dataset = tf.data.Dataset.from_tensor_slices(positive_images)\n",
        "negative_dataset = tf.data.Dataset.from_tensor_slices(negative_images)\n",
        "\n",
        "dataset = tf.data.Dataset.zip((anchor_dataset, positive_dataset, negative_dataset))\n",
        "dataset = dataset.shuffle(buffer_size=1024)\n",
        "dataset = dataset.map(preprocess_triplets)\n",
        "\n",
        "train_dataset = dataset.take(round(image_count * 0.8))\n",
        "val_dataset = dataset.skip(round(image_count * 0.8))\n",
        "\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
        "train_dataset = train_dataset.prefetch(8)\n",
        "\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
        "val_dataset = val_dataset.prefetch(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXnfe1A5tPaP"
      },
      "outputs": [],
      "source": [
        "visualize(*list(train_dataset.take(11).as_numpy_iterator())[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svz34hLxtdjU"
      },
      "outputs": [],
      "source": [
        "callbacks = [EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYWqI6xct300"
      },
      "outputs": [],
      "source": [
        "siamese_model = SiameseModel(siamese_network)\n",
        "siamese_model.compile(optimizer=optimizers.Adam(LEARNING_RATE))\n",
        "siamese_model.fit(train_dataset, epochs=EPOCHS, validation_data=val_dataset, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8gbbupKJ_Na"
      },
      "outputs": [],
      "source": [
        "sample = next(iter(train_dataset))\n",
        "anchor, positive, negative = sample\n",
        "anchor_embedding, positive_embedding, negative_embedding = (embedding(resnet.preprocess_input(anchor)), embedding(resnet.preprocess_input(positive)), embedding(resnet.preprocess_input(negative)),)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUBj3j3NLYyO"
      },
      "outputs": [],
      "source": [
        "anchors, positives, negatives = sample\n",
        "for i in range(len(anchor_embedding)):\n",
        "    anchor, positive, negative = anchors[i], positives[i], negatives[i]\n",
        "    positive_distance = np.linalg.norm(anchor_embedding[i] - positive_embedding[i], ord=2)*1e4\n",
        "    negative_distance = np.linalg.norm(anchor_embedding[i] - negative_embedding[i], ord=2)*1e4\n",
        "    positive_distance, negative_distance = round(positive_distance)/1e4, round(negative_distance)/1e4\n",
        "    plt.figure(figsize=(16,9))\n",
        "    plt.subplot(131)\n",
        "    plt.imshow(anchor)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Anchor\")\n",
        "    plt.subplot(132)\n",
        "    plt.imshow(positive)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Anchor to Positive Distance: {positive_distance}\")\n",
        "    plt.subplot(133)\n",
        "    plt.imshow(negative)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Anchor to Negative Distance: {negative_distance}\")\n",
        "    plt.show()   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT0LmuFNLuzD"
      },
      "outputs": [],
      "source": [
        "embedding.save(\"bottomwear_embedding.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8vsy1dnLxhK"
      },
      "outputs": [],
      "source": [
        "!cp bottomwear_embedding.h5 /content/drive/MyDrive/shop_the_look/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlvt-KcSMAy3"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Train-Footwear </b> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdOUYhw_MCwx"
      },
      "outputs": [],
      "source": [
        "dataframe = pd.read_csv(\"foot_pairs.csv\")\n",
        "dataframe.negative.fillna(dataframe.negative.mode()[0], inplace=True)\n",
        "for column in dataframe.columns:\n",
        "    dataframe[column] = dataframe[column].apply(lambda x: x.replace(\"\\\\\",\"/\")) \n",
        "\n",
        "anchor_images = dataframe.anchor.to_list()\n",
        "positive_images = dataframe.positive.to_list()\n",
        "negative_images = dataframe.negative.to_list()\n",
        "image_count = len(anchor_images)\n",
        "\n",
        "anchor_dataset = tf.data.Dataset.from_tensor_slices(anchor_images)\n",
        "positive_dataset = tf.data.Dataset.from_tensor_slices(positive_images)\n",
        "negative_dataset = tf.data.Dataset.from_tensor_slices(negative_images)\n",
        "\n",
        "dataset = tf.data.Dataset.zip((anchor_dataset, positive_dataset, negative_dataset))\n",
        "dataset = dataset.shuffle(buffer_size=1024)\n",
        "dataset = dataset.map(preprocess_triplets)\n",
        "\n",
        "train_dataset = dataset.take(round(image_count * 0.8))\n",
        "val_dataset = dataset.skip(round(image_count * 0.8))\n",
        "\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
        "train_dataset = train_dataset.prefetch(8)\n",
        "\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
        "val_dataset = val_dataset.prefetch(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5qRRJhwMJhC"
      },
      "outputs": [],
      "source": [
        "visualize(*list(train_dataset.take(2).as_numpy_iterator())[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-mmG1vCMLjv"
      },
      "outputs": [],
      "source": [
        "callbacks = [EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mNFpoCwMMEb"
      },
      "outputs": [],
      "source": [
        "siamese_model = SiameseModel(siamese_network)\n",
        "siamese_model.compile(optimizer=optimizers.Adam(LEARNING_RATE))\n",
        "siamese_model.fit(train_dataset, epochs=EPOCHS, validation_data=val_dataset, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "946SF7vSMN33"
      },
      "outputs": [],
      "source": [
        "sample = next(iter(train_dataset))\n",
        "anchor, positive, negative = sample\n",
        "anchor_embedding, positive_embedding, negative_embedding = (embedding(resnet.preprocess_input(anchor)), embedding(resnet.preprocess_input(positive)), embedding(resnet.preprocess_input(negative)),)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjJXnfREMSiW"
      },
      "outputs": [],
      "source": [
        "anchors, positives, negatives = sample\n",
        "for i in range(len(anchor_embedding)):\n",
        "    anchor, positive, negative = anchors[i], positives[i], negatives[i]\n",
        "    positive_distance = np.linalg.norm(anchor_embedding[i] - positive_embedding[i], ord=2)*1e4\n",
        "    negative_distance = np.linalg.norm(anchor_embedding[i] - negative_embedding[i], ord=2)*1e4\n",
        "    positive_distance, negative_distance = round(positive_distance)/1e4, round(negative_distance)/1e4\n",
        "    plt.figure(figsize=(16,9))\n",
        "    plt.subplot(131)\n",
        "    plt.imshow(anchor)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Anchor\")\n",
        "    plt.subplot(132)\n",
        "    plt.imshow(positive)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Anchor to Positive Distance: {positive_distance}\")\n",
        "    plt.subplot(133)\n",
        "    plt.imshow(negative)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Anchor to Negative Distance: {negative_distance}\")\n",
        "    plt.show()   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiQPkIw8MYbN"
      },
      "outputs": [],
      "source": [
        "embedding.save(\"footwear_embedding.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRLrdKi7Macg"
      },
      "outputs": [],
      "source": [
        "!cp footwear_embedding.h5 /content/drive/MyDrive/shop_the_look/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kbkMxIWLpMf"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Embedding-Generation(Inference) </b> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tK_iW5Yj4sR"
      },
      "source": [
        "* Now it is time to generate the embeddings for our database/catologue Skroutz images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BizzoOEzcyA_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.applications import resnet\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from skimage import io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1zNjmOxkULs"
      },
      "source": [
        "* We will create the `preprocess_image` function to load the specified file as a JPEG image, preprocess it and resize it to the target shape. `generate_embedding` function uses the specified dataframe and embeddings model to calculate embeddings for all records in the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upKapm2mb3xn"
      },
      "outputs": [],
      "source": [
        "TARGET_SHAPE = (224,224,3)\n",
        "def preprocess_image(filename: str):\n",
        "    image_string = tf.io.read_file(filename)\n",
        "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, TARGET_SHAPE[:2])\n",
        "    image = resnet.preprocess_input(image)\n",
        "    return tf.expand_dims(image, axis=0)\n",
        "\n",
        "\n",
        "def generate_embedding(dataframe, embedding_generator):\n",
        "    embeddings = list()\n",
        "    for i, row in tqdm(dataframe.iterrows(), total=dataframe.shape[0]):\n",
        "        filename = row[\"image_path\"]\n",
        "        image = preprocess_image(filename)\n",
        "        embedding = embedding_generator(image)\n",
        "        embeddings.append(embedding[0].numpy().astype(np.float32).tolist())\n",
        "    dataframe[\"embedding\"] = embeddings\n",
        "    return dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJz74udeLpMf"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Skroutz.gr Images Dataframe </b> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23GR14XHk7mo"
      },
      "source": [
        "* Below is the Skrout images dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBmfTQPadZoN"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/shop_the_look/skroutz_images.csv', usecols = ['product_id','description','brand','gender','category','category_name'])\n",
        "\n",
        "conditions = [\n",
        "    (df['gender'] == 'women') & (df['category'] == 'topwear'),\n",
        "    (df['gender'] == 'women') & (df['category'] == 'bottomwear'),\n",
        "    (df['gender'] == 'women') & (df['category'] == 'footwear'),\n",
        "    (df['gender'] == 'men') & (df['category'] == 'topwear'),\n",
        "    (df['gender'] == 'men') & (df['category'] == 'bottomwear'),\n",
        "    (df['gender'] == 'men') & (df['category'] == 'footwear')]\n",
        "\n",
        "choices = ['wtw', 'wbw', 'wft','mtw','mbw','mfw']\n",
        "\n",
        "df['code'] = np.select(conditions, choices)\n",
        "\n",
        "df['image_path'] = df['gender'] + '_' + df['category'] + '/' + df['code'] + '_' + df['product_id'].map(str) + '.jpeg'\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDsU7oeXLpMg"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Dataframe Split </b> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KutyTNiMlAsz"
      },
      "source": [
        "* We split the dataframe according to the gender and category columns in separate dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P736yQncE6V"
      },
      "outputs": [],
      "source": [
        "mens_topwear = df[(df[\"gender\"] == \"men\") & (df[\"category\"] == \"topwear\")]\n",
        "mens_bottomwear = df[(df[\"gender\"] == \"men\") & (df[\"category\"] == \"bottomwear\")]\n",
        "mens_footwear = df[(df[\"gender\"] == \"men\") & (df[\"category\"] == \"footwear\")]\n",
        "womens_topwear = df[(df[\"gender\"] == \"women\") & (df[\"category\"] == \"topwear\")]\n",
        "womens_bottomwear = df[(df[\"gender\"] == \"women\") & (df[\"category\"] == \"bottomwear\")]\n",
        "womens_footwear = df[(df[\"gender\"] == \"women\") & (df[\"category\"] == \"footwear\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn-zv1kQLpMg"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Loading Embedding Models</b> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIfdjrq6lL4T"
      },
      "source": [
        "* We load the trained embedding models "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9fyS0JscJRt"
      },
      "outputs": [],
      "source": [
        "topwear_embedding = load_model(\"/content/drive/MyDrive/shop_the_look/topwear_embedding.h5\")\n",
        "bottomwear_embedding = load_model(\"/content/drive/MyDrive/shop_the_look/bottomwear_embedding.h5\")\n",
        "footwear_embedding = load_model(\"/content/drive/MyDrive/shop_the_look/footwear_embedding.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bJv4hUclUhJ"
      },
      "source": [
        "* We apply the `generate_embedding` function and save the embeddings to .csv files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-cpt9fxcYuv"
      },
      "outputs": [],
      "source": [
        "womens_topwear = generate_embedding(womens_topwear, topwear_embedding)\n",
        "womens_topwear.to_csv(\"/content/drive/MyDrive/shop_the_look/womens_topwear_embeddings.csv\", index=False)\n",
        "womens_topwear = pd.read_csv(\"/content/drive/MyDrive/shop_the_look/womens_topwear_embeddings.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd3jgKJjfcRl"
      },
      "outputs": [],
      "source": [
        "womens_bottomwear = generate_embedding(womens_bottomwear, bottomwear_embedding)\n",
        "womens_bottomwear.to_csv(\"/content/drive/MyDrive/shop_the_look/womens_bottomwear_embeddings.csv\", index=False)\n",
        "womens_bottomwear = pd.read_csv(\"/content/drive/MyDrive/shop_the_look/womens_bottomwear_embeddings.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYkIQxxXgPV_"
      },
      "outputs": [],
      "source": [
        "womens_footwear = generate_embedding(womens_footwear, footwear_embedding)\n",
        "womens_footwear.to_csv(\"/content/drive/MyDrive/shop_the_look/womens_footwear_embeddings.csv\", index=False)\n",
        "womens_footwear = pd.read_csv(\"/content/drive/MyDrive/shop_the_look/womens_footwear_embeddings.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEfAgkfNgUop"
      },
      "outputs": [],
      "source": [
        "mens_topwear = generate_embedding(mens_topwear, topwear_embedding)\n",
        "mens_topwear.to_csv(\"/content/drive/MyDrive/shop_the_look/mens_topwear_embeddings.csv\", index=False)\n",
        "mens_topwear = pd.read_csv(\"/content/drive/MyDrive/shop_the_look/mens_topwear_embeddings.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEBvxtzWjPGL"
      },
      "outputs": [],
      "source": [
        "mens_bottomwear = generate_embedding(mens_bottomwear, bottomwear_embedding)\n",
        "mens_bottomwear.to_csv(\"/content/drive/MyDrive/shop_the_look/mens_bottomwear_embeddings.csv\", index=False)\n",
        "mens_bottomwear = pd.read_csv(\"/content/drive/MyDrive/shop_the_look/mens_bottomwear_embeddings.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7hpYmy7jQyj"
      },
      "outputs": [],
      "source": [
        "mens_footwear = generate_embedding(mens_footwear, footwear_embedding)\n",
        "mens_footwear.to_csv(\"/content/drive/MyDrive/shop_the_look/mens_footwear_embeddings.csv\", index=False)\n",
        "mens_footwear = pd.read_csv(\"/content/drive/MyDrive/shop_the_look/mens_footwear_embeddings.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGrhsaQ1mBDd"
      },
      "outputs": [],
      "source": [
        "womens_topwear = pd.read_csv(\"/content/drive/MyDrive/shop_the_look/womens_topwear_embeddings.csv\")\n",
        "womens_bottomwear = pd.read_csv(\"/content/drive/MyDrive/shop_the_look/womens_bottomwear_embeddings.csv\")\n",
        "womens_footwear = pd.read_csv(\"/content/drive/MyDrive/shop_the_look/womens_footwear_embeddings.csv\")\n",
        "mens_topwear = pd.read_csv(\"/content/drive/MyDrive/shop_the_look/mens_topwear_embeddings.csv\")\n",
        "mens_bottomwear = pd.read_csv(\"/content/drive/MyDrive/shop_the_look/mens_bottomwear_embeddings.csv\")\n",
        "mens_footwear = pd.read_csv(\"/content/drive/MyDrive/shop_the_look/mens_footwear_embeddings.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0RJ4RfSLpMh"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Images with Minimum Distance </b> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d7Vwjbelpfu"
      },
      "source": [
        "* We now pick a random image from the dataset and find the images with the minimum distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQR3cueSjd1C"
      },
      "outputs": [],
      "source": [
        "tqdm.pandas()\n",
        "query_embedding = womens_topwear.iloc[random.randint(0,womens_topwear.shape[0])].embedding\n",
        "womens_topwear['distance'] = womens_topwear['embedding'].progress_apply(lambda x: np.linalg.norm(np.asarray(eval(x), dtype=np.float32) - np.asarray(eval(query_embedding), dtype=np.float32)))\n",
        "womens_topwear = womens_topwear.sort_values(by='distance').reset_index(drop=True)\n",
        "plt.figure(figsize=(16,9))\n",
        "plt.imshow(io.imread(womens_topwear.iloc[0].image_path))\n",
        "plt.title(\"Query Image\")\n",
        "plt.axis(\"off\");plt.show()\n",
        "print(254*\"=\")\n",
        "print(254*\"=\")\n",
        "for i, row in womens_topwear.iloc[1:11].iterrows():\n",
        "    plt.figure(figsize=(16,9))\n",
        "    image = io.imread(row[\"image_path\"])\n",
        "    plt.imshow(image);plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    print(254*\"=\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXZvdXRvLpMh"
      },
      "source": [
        "***\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LstMasFk9C8"
      },
      "source": [
        "<a class=\"anchor\" id=\"6\"></a> \n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://i.ibb.co/82xghNf/pipeline.png\" alt=\"Final Pipeline\" style=\"width: 660px;\"/><br /> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCjdfq8RLpMh"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Combining All Modules Together </b> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yIrBjfImUg-"
      },
      "source": [
        "* The final step is to combine all the modules into a pipeline that takes in a query image and outputs the relevant recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGkcs08Tk6SY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.applications import resnet\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import io\n",
        "%matplotlib inline\n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESeUJCjX57tn"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcPqBDxUlCxa"
      },
      "outputs": [],
      "source": [
        "LOADED_MODELS = dict()\n",
        "LOADED_CSVS = dict()\n",
        "TARGET_SHAPE = (224,224,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbd4HStMLpMi"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Functions </b> \n",
        "* Explanations:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1ZHTYKimgPT"
      },
      "source": [
        "\n",
        "\n",
        "> *   `preprocess_image1` : Loads the specified file as a JPEG image, preprocess it and resize it to the target shape for Gender Classification\n",
        ">\n",
        "> *   `preprocess_image2` : Loads the specified file as a JPEG image, preprocess it and resize it to the target shape for Embedding Generation\n",
        ">\n",
        ">*   `load_models` : Loads gender classifier model , and embeddings generaton models for the recommendation engine pipeline\n",
        ">\n",
        ">*   `load_CSVs` :  Loads all the Skroutz images embeddings stored in CSV files\n",
        ">\n",
        ">*   `extract_clothes` : Extracts Topwear, Bottomwear and Footwear from a given Image\n",
        ">\n",
        ">*   `plot_clothes` : Plots the extracted clothes of the given image\n",
        ">\n",
        ">*  `generate_embedding` : Generate the embeddings for cropped outputs\n",
        ">\n",
        ">*  `__get__` :  Plots top 3 similar products for each category\n",
        ">\n",
        ">*   `get_results` : get Similar products for a given input containing query product\n",
        ">\n",
        ">*   `final` : The complete pipeline for recommending similar fashion products based on a query image.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD_JO3KAlHHQ"
      },
      "outputs": [],
      "source": [
        "def preprocess_image1(filename):\n",
        "    image_string = tf.io.read_file(filename)\n",
        "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
        "    image = tf.image.resize(image, TARGET_SHAPE[:2])\n",
        "    return tf.expand_dims(image, axis=0)\n",
        "\n",
        "def preprocess_image2(filename):\n",
        "    image_string = tf.io.read_file(filename)\n",
        "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, TARGET_SHAPE[:2])\n",
        "    image = resnet.preprocess_input(image)\n",
        "    return tf.expand_dims(image, axis=0)\n",
        "\n",
        "def load_models():\n",
        "    if len(LOADED_MODELS) == 4:\n",
        "        return True\n",
        "    else:\n",
        "        print(\"Loading Models...\")\n",
        "        GENDER_CLASSIFIER = load_model(\"/content/drive/MyDrive/shop_the_look/gender_classification_model.h5\")\n",
        "        TOPWEAR_EMBEDDING = load_model(\"/content/drive/MyDrive/shop_the_look/topwear_embedding.h5\")\n",
        "        BOTTOMWEAR_EMBEDDING = load_model(\"/content/drive/MyDrive/shop_the_look/bottomwear_embedding.h5\")\n",
        "        FOOTWEAR_EMBEDDING = load_model(\"/content/drive/MyDrive/shop_the_look/footwear_embedding.h5\")\n",
        "\n",
        "        LOADED_MODELS[\"gender_classifier\"] = GENDER_CLASSIFIER\n",
        "        LOADED_MODELS[\"topwear\"] = TOPWEAR_EMBEDDING\n",
        "        LOADED_MODELS[\"bottomwear\"] = BOTTOMWEAR_EMBEDDING\n",
        "        LOADED_MODELS[\"footwear\"] = FOOTWEAR_EMBEDDING\n",
        "        print(\"Models Loaded!\")\n",
        "        return True\n",
        "    \n",
        "def load_CSVs():\n",
        "    if len(LOADED_CSVS) == 6:\n",
        "        return True\n",
        "    else:\n",
        "        print(\"Loading CSVs...\")\n",
        "        MENS_TOPWEAR_CSV = pd.read_csv(\"/content/drive/MyDrive/shop_the_look/mens_topwear_embeddings.csv\")\n",
        "        MENS_BOTTOMWEAR_CSV = pd.read_csv(\"/content/drive/MyDrive/shop_the_look/mens_bottomwear_embeddings.csv\")\n",
        "        MENS_FOOTWEAR_CSV = pd.read_csv(\"/content/drive/MyDrive/shop_the_look/mens_footwear_embeddings.csv\")\n",
        "        WOMENS_TOPWEAR_CSV = pd.read_csv(\"/content/drive/MyDrive/shop_the_look/womens_topwear_embeddings.csv\")\n",
        "        WOMENS_BOTTOMWEAR_CSV = pd.read_csv(\"/content/drive/MyDrive/shop_the_look/womens_bottomwear_embeddings.csv\")\n",
        "        WOMENS_FOOTWEAR_CSV = pd.read_csv(\"/content/drive/MyDrive/shop_the_look/womens_footwear_embeddings.csv\")\n",
        "        \n",
        "        LOADED_CSVS[\"mens_topwear\"] = MENS_TOPWEAR_CSV\n",
        "        LOADED_CSVS[\"mens_bottomwear\"] = MENS_BOTTOMWEAR_CSV\n",
        "        LOADED_CSVS[\"mens_footwear\"] = MENS_FOOTWEAR_CSV\n",
        "        LOADED_CSVS[\"womens_topwear\"] = WOMENS_TOPWEAR_CSV\n",
        "        LOADED_CSVS[\"womens_bottomwear\"] = WOMENS_BOTTOMWEAR_CSV\n",
        "        LOADED_CSVS[\"womens_footwear\"] = WOMENS_FOOTWEAR_CSV\n",
        "        print(\"CSVs Loaded!\")\n",
        "        return True\n",
        "    \n",
        "def extract_clothes(img_path: str): \n",
        "    import torch\n",
        "    %cd yolov5\n",
        "    model = torch.hub.load('.', 'custom', path='/content/drive/MyDrive/shop_the_look/object_detection.pt', source='local') \n",
        "    image = plt.imread(img_path)\n",
        "    results = model(image)\n",
        "    topwear_score, bottomwear_score, footwear_score = 0.5, 0.5, 0.5\n",
        "    outputs = {\"original_image\": image}\n",
        "    for index, row in results.pandas().xyxy[0].iterrows():\n",
        "      XMIN = int(row['xmin'])\n",
        "      YMIN = int(row['ymin']) \n",
        "      XMAX = int(row['xmax'])\n",
        "      YMAX = int(row['ymax'])\n",
        "      if (row['class'] == 0) and (row['confidence'] > topwear_score) :\n",
        "          outputs[\"topwear\"] = image[YMIN:YMAX, XMIN:XMAX]\n",
        "      elif (row['class'] == 1) and (row['confidence'] > bottomwear_score):\n",
        "          outputs[\"bottomwear\"] = image[YMIN:YMAX, XMIN:XMAX]\n",
        "      elif (row['class'] == 2) and (row['confidence'] > footwear_score):\n",
        "          outputs[\"footwear\"] = image[YMIN:YMAX, XMIN:XMAX]\n",
        "    %cd '/content/drive/MyDrive/shop_the_look'\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def plot_clothes(**images):\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "      plt.subplot(1, n, i + 1)\n",
        "      plt.axis(\"off\")\n",
        "      plt.title(' '.join(name.split('_')).title())\n",
        "      plt.imshow(image)\n",
        "    plt.show()\n",
        "    \n",
        "def generate_embedding(outputs, detected_objects):\n",
        "    for output in detected_objects:\n",
        "        image = outputs[output]\n",
        "        img_path = os.path.join(\"/content/\",\"example.jpg\")\n",
        "        saved_image = Image.fromarray(image)\n",
        "        saved_image.save(img_path)\n",
        "        image = preprocess_image2(img_path)\n",
        "        embedding = LOADED_MODELS[output](image)[0].numpy().astype(np.float32).tolist()\n",
        "        outputs[output+\"_embedding\"] = embedding\n",
        "        os.remove(img_path)\n",
        "    return outputs\n",
        "\n",
        "def __get__(csv_file, query):\n",
        "    csv_file['distance'] = csv_file['embedding'].apply(lambda x: np.linalg.norm(np.asarray(eval(x), dtype=np.float32) - np.asarray(query, dtype=np.float32)))\n",
        "    csv_file = csv_file.sort_values(by='distance').reset_index(drop=True)\n",
        "    result = csv_file.iloc[:3][['product_id', 'image_path']].to_dict('records')\n",
        "    for i, row in csv_file.iloc[:3].iterrows():\n",
        "        plt.figure(figsize=(16,9))\n",
        "        image = io.imread(\"/content/drive/MyDrive/shop_the_look/\"+row[\"image_path\"])\n",
        "        plt.imshow(image);plt.axis(\"off\");plt.show()\n",
        "        #print(\"Shop Now @ \", row[\"product_id\"])\n",
        "        print(254*\"=\")\n",
        "    return result\n",
        "\n",
        "def get_results(outputs, gender, detected_objects):\n",
        "    dict_results = dict()\n",
        "    for output in detected_objects:\n",
        "        csv_file = gender + \"_\" + output\n",
        "        csv_file = LOADED_CSVS[csv_file].copy(deep=True)\n",
        "        query = outputs[output+\"_embedding\"]\n",
        "        dict_results[output] = __get__(csv_file, query)\n",
        "    return dict_results\n",
        "\n",
        "def final(images):\n",
        "    if isinstance(images, str):\n",
        "        images = [images]\n",
        "    if load_models() and load_CSVs():\n",
        "        for img_path in images:\n",
        "            inputs = preprocess_image1(img_path)\n",
        "            gender_score = LOADED_MODELS[\"gender_classifier\"](inputs)[0][0].numpy()\n",
        "            gender = \"mens\" if  gender_score < 0.5 else \"womens\"\n",
        "            print(gender)\n",
        "            outputs = extract_clothes(img_path)\n",
        "            detected_objects = [k for k in outputs if k != \"original_image\"]\n",
        "            plot_clothes(**outputs)\n",
        "            outputs = generate_embedding(outputs, detected_objects)\n",
        "            results = get_results(outputs, gender, detected_objects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy-7rK20LpMi"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>Testing the Pipeline </b> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EogfH9siok5J"
      },
      "source": [
        "* Lets now test the whole pipeline by giving as input random images from the web ( test_image_1.jpg, test_image_2.jpeg, test_image_3.jpg, test_image_4.jpg ) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b07r6M7GLpMi"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>| Example A </b> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKwbPZYJoW_-"
      },
      "outputs": [],
      "source": [
        "img_path = \"/content/drive/MyDrive/shop_the_look/test_image_1.jpg\"\n",
        "final(img_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thy2BMvCLpMi"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>| Example B </b> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oJdGtcAvhpn"
      },
      "outputs": [],
      "source": [
        "img_path = \"/content/drive/MyDrive/shop_the_look/test_image_2.jpeg\"\n",
        "final(img_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoo3TweVLpMi"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>| Example C </b> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoQoOXaMwm38"
      },
      "outputs": [],
      "source": [
        "img_path = \"/content/drive/MyDrive/shop_the_look/test_image_3.jpg\"\n",
        "final(img_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvvO114wLpMj"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>| Example D </b> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvp-_0d545Ks"
      },
      "outputs": [],
      "source": [
        "img_path = \"/content/drive/MyDrive/shop_the_look/test_image_4.jpg\"\n",
        "final(img_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dwXgvytLpMj"
      },
      "source": [
        "<b> <font size=\"+3\"><span style='font-family:Verdana '>| Example E </b> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-A9QEX3vxnQ"
      },
      "outputs": [],
      "source": [
        "img_path = \"/content/drive/MyDrive/shop_the_look/men_topwear/mtw_29222194.jpeg\"\n",
        "final(img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vASnSoXGLpMj"
      },
      "outputs": [],
      "source": [
        "%%html \n",
        "<font size=\"+5\">\n",
        "<marquee style='width: 90%; color: orange;'><b>üôè THE END ~ WHISKEY TEAM ~ 2022 ~ SHOP THE LOOK üôè</b></marquee>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqI8zsEULpMj"
      },
      "source": [
        "***\n",
        "\n",
        "[Back to Top ‚Üë](#00) <img style=\"float: right;\" src=\"https://i.ibb.co/w7Lbd0X/2-AUEB-white-HR.jpg\"  alt=\"aueb\" width=\"550px\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I34cl3vpLpMj"
      },
      "outputs": [],
      "source": [
        "‚ñë‚ñë‚ñë‚îå‚îê‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îå‚îê‚îå‚îê‚ñë‚ñë‚ñë‚ñë‚ñë‚îå‚îê‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îå‚îê‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îå‚îê‚ñë‚ñë\n",
        "‚ñë‚ñë‚ñë‚îÇ‚îÇ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îå‚îò‚îî‚î§‚îÇ‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ‚îÇ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ‚îÇ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ‚îÇ‚ñë‚ñë\n",
        "‚îå‚îÄ‚îÄ‚î§‚îî‚îÄ‚î¨‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îê‚îî‚îê‚îå‚î§‚îî‚îÄ‚î¨‚îÄ‚îÄ‚îê‚îÇ‚îÇ‚ñë‚ñë‚îå‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚î§‚îÇ‚îå‚îê‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îå‚îÄ‚îÄ‚î¨‚îê‚îå‚î¨‚îÄ‚îÄ‚î§‚îî‚îÄ‚îê\n",
        "‚îÇ‚îÄ‚îÄ‚î§‚îå‚îê‚îÇ‚îå‚îê‚îÇ‚îå‚îê‚îÇ‚ñë‚îÇ‚îÇ‚îÇ‚îå‚îê‚îÇ‚îÇ‚îÄ‚î§‚îÇ‚îÇ‚ñë‚îå‚î§‚îå‚îê‚îÇ‚îå‚îê‚îÇ‚îî‚îò‚îò‚îå‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îê‚îÇ‚îå‚îê‚îÇ‚îÇ‚îÇ‚îÇ‚îÇ‚îÄ‚î§‚îå‚îê‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÇ‚îÇ‚îÇ‚îÇ‚îî‚îò‚îÇ‚îî‚îò‚îÇ‚ñë‚îÇ‚îî‚î§‚îÇ‚îÇ‚îÇ‚îÇ‚îÄ‚î§‚îÇ‚îî‚îÄ‚îò‚îÇ‚îî‚îò‚îÇ‚îî‚îò‚îÇ‚îå‚îê‚îê‚îî‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îò‚îÇ‚îå‚îê‚îÇ‚îî‚îò‚îÇ‚îÇ‚îÄ‚î§‚îî‚îò‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚î¥‚îò‚îî‚î¥‚îÄ‚îÄ‚î§‚îå‚îÄ‚îò‚ñë‚îî‚îÄ‚î¥‚îò‚îî‚î¥‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¥‚îò‚îî‚îò‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îî‚îò‚îî‚î¥‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îò\n",
        "‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ‚îÇ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
        "‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îî‚îò‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Shop_The_Look_Despotis-Papailiou_NO_OUTPUT.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}